---
title: "Deploying a large language model (LLM) on my local machine"
date: 2024-11-09
permalink: /posts/2024-00-00/new-blog
categories:
  - Misc
tags:
  - LLM
toc: true
# last_modified_at: 2024-09-01
---

I am planing to deploy a large language model (LLM) on my local machine (AMD-ITX). The machine is just a personal computer running on ubuntu, coming with a 8-core CPU (AMD Ryzen 7 5700X), 32G RAM and a GPU of RTX 4060ti 16G. Only small size LLMs are suitable. Here I record the process of deploying a LLM on my local machine, for the ease of future maintenance.

Some important references: 

1. [a online tutorial in Chinese](https://cuterwrite.top/p/integrate-open-webui-ollama-qwen25-local-rag/)
2. [another turotial in English](https://stevescargall.com/blog/2024/05/running-open-webui-and-ollama-on-ubuntu-22.04-for-a-local-chatgpt-experience/)
3. [Docker](https://docs.docker.com/engine/install/ubuntu/)
4. [NVidia Container Runtime for Docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#prerequisites)
5. [Cloudflared](https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/get-started/create-remote-tunnel/)

## Install Docker

### Start Docker

```bash
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
```

### NVidia Container Runtime for Docker

```bash
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

sudo nvidia-ctk runtime configure --runtime=docker # The nvidia-ctk command modifies the /etc/docker/daemon.json file on the host. The file is updated so that Docker can use the NVIDIA Container Runtime.
sudo systemctl restart docker
```

## Install Open WebUI 

With NVidia GPU and CUDA Support: Utilize GPU resources by running the following command:
```bash
docker run -d -p 4000:8080 \
        --gpus all \
        --add-host=host.docker.internal:host-gateway \
        -v /DATA/open-webui:/app/backend/data \
        --name open-webui \
        --restart always \
        ghcr.io/open-webui/open-webui:cuda

```
Verify the Docker instance is running:
```bash
sudo docker ps
```

Now the Open WebUI is running on your local machine. You can access it by visiting `http://localhost:4000` in your browser.

## Install and start Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh # install
ollama serve # start for once
```

### start Ollama as a service

First change the OLLAMA_HOST to 0.0.0.0 referring to: https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux

then `sudo systemctl enable ollama` and `sudo systemctl start ollama`

## Expose the Open WebUI to the Internet via cloudflared

1. Log in to [Zero Trust](https://one.dash.cloudflare.com/), and go to Networks > Tunnels.
2. Select the tunnel (AMD-ITX here) and add a public hostname (e.g., "local_llm"), specifying appropriate port (4000 as above). 
3. Under the "Access > Applications" tab, add an application and select the appropriate access policy.
4. Now visit https://local_llm.guhaogao.com and you are good to go.

## Downloading Ollama Models

1. [Qwen2.5-14B_Uncensored_Instruct-Q6_K_L](https://huggingface.co/bartowski/Qwen2.5-14B_Uncensored_Instruct-GGUF/resolve/main/Qwen2.5-14B_Uncensored_Instruct-Q6_K_L.gguf)
2. [Qwen2.5-32B-Instruct-IQ3_M](https://huggingface.co/bartowski/Qwen2.5-32B-Instruct-GGUF/resolve/main/Qwen2.5-32B-Instruct-IQ3_M.gguf)
3. llama3.2-vision:11b
4. [Llama-3.1-8B-Instruct-Q6_K_L](https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf)
5. bge-m3:latest
6. [gemma-2-9b-it-Q6_K_L](https://huggingface.co/bartowski/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q6_K_L.gguf)
7. [gemma-2-27b-it-IQ4_XS](https://huggingface.co/bartowski/gemma-2-27b-it-GGUF/resolve/main/gemma-2-27b-it-IQ4_XS.gguf)
8. mistral-small
9. [Mistral-Nemo-Instruct-2407-Q6_K_L](https://huggingface.co/bartowski/Mistral-Nemo-Instruct-2407-GGUF/resolve/main/Mistral-Nemo-Instruct-2407-Q6_K_L.gguf)

---

