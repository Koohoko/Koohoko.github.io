---
title: "Course notes: Machine Learning (Lee Hung-yi, 2016)"
date: 2022-12-17
permalink: /posts/2022-12-17/ml-ntu-2016
categories:
  - Course notes
tags:
  - Machine learning
toc: true
last_modified_at: 2022-12-17
---

This post records the notes when I learnt the [machine learning course from Lee Hung-yi](https://www.youtube.com/playlist?list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49). 

## 0: Introduction of Machine Learning
The framework of machine learning can be summarized into three steps:
  1. A set of functions (Model);
  2. Goodness of function $f$ (build a function to evaluate the functions in step 1, e.g. Loss function);
  3. Pick the best function (using training data (supervised learning, structured learning), or without training data (unsupervised training), or some combination (semi-supervised learning, transfer learning, reinforcement training)) and testing.
  
> *让我们朝着AI训练师的方向进发吧！*

## 1: Regression
- Gradient descent: 只要可微分的function就可以用gradient descent找到local minima. 如果有多个参数，就对每个参数（每一个dimension）做偏微分。But in linear regression, the Loss function is *convex*.
- Regularization: Redefined the loss function to make the function more smooth (the functions with smaller $w_i$ are better, therefore are insensitive to noise). 
- Learning rate: we can specify customized learning rate for different parameters, e.g. *Adagrad Optimizer*.

## 2: Where dose the error come from
- There are two types of error : 1. from bias (the difference between the true function $\hat{f}$ and our estimated function $f^*$) and 2. from variance (the variance of the estimated functions).
  - Overfitting: Error from variance is too big. You should try to collect more training data, or use *regularization*.
  - Underfitting: Error from bias is too big. You should redesign your model (add more features or use a more complex model) to fit the training sets.

- Cross-validation: We should only use training set to choose your model, don't cherry-picking with the testing set. <img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-18%20at%2014.22.25.webp" width="500"/>
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-18%20at%2014.34.06.webp" width="500"/>

## 3: Gradient descent
### Adaptive learning rate
We can use adaptive learning rate to better approach the local minima. For example in the below figure, $\eta^t$ depends on the number of epochs $t$, and $\sigma^t$ depends on (all) the previous derivatives.

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-18%20at%2020.03.19.webp" width="500"/>

The denominator in adagrad formula estimates the *second derivative* of the gradient. We need to account for the second derivative because the best step should also consider the second derivative (see below: although the points in $w2$ have bigger first derivatives than data points in $w1$, they in fact are more close to the local minima).

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2000.18.18.webp" width="500"/>

### Stochastic gradient descent
Loss for only one sample, the advantage is it can update very frequently.

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2000.41.57.webp" width="500"/>

### Feature scaling
Make different features have the same scaling will help finding the local minima more efficiently.

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2000.52.54.webp" width="500"/>

## 4. Classification
In classification problems, the outputs are class/categories. 

### Naive regression
To solve a classification problem, one way (probably in instinct) is to use the regression. Specifically, you can label one class as $1$ and the other class as $-1$, then find the best parameters $w_1$ and $w_2$ in the model $y=b+w_1x_1+w_2x_2$. However, there are some problem with this naive approach, that it is sensitive to outlier values, and it is hard to deal with multiple classes:

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2018.24.09.webp" width="500"/>

### Probability generative model
与生成模型(generative model)对应的是决断模型(descrimintive model). 类似regression, SVM这样的模型属于决断模型。而另一种思路是生成模型，即生成每个class的概率分布，然后对比新的feature在每个class的概率分布里面的可能性(e.g., $P(x|C_1)$)的大小。

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2021.06.04.webp" width="500"/>

三步建立概率生成模型

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2023.36.40.webp" width="500"/>

如果不同参数之间是independent的，这种情况下构成一个Naive Bayes Classifier. 
而当不同的class的概率分布share同一个covariance的时候，这是一个"linear model" (2-D的时候, 判别线为直线). 2-D下, 当$\Epsilon_1=\Epsilon_2$时，其linearity可见下面的推导:

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2023.45.51.webp" width="500"/>

> *中间步骤省略*

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2023.53.42.webp" width="500"/>

## 5. Logistic Regression