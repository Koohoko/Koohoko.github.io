---
title: "Course notes: Machine Learning (Lee Hung-yi, 2016)"
date: 2022-12-17
permalink: /posts/2022-12-17/ml-ntu-2016
categories:
  - Course notes
tags:
  - Machine learning
toc: true
last_modified_at: 2022-12-17
---

This post records the notes when I learnt the [machine learning course from Lee Hung-yi](https://www.youtube.com/playlist?list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49). 

## 0: Introduction of Machine Learning
The framework of machine learning can be summarized into three steps:
  1. A set of functions (Model);
  2. Goodness of function $f$ (build a function to evaluate the functions in step 1, e.g. Loss function);
  3. Pick the best function (using training data (supervised learning, structured learning), or without training data (unsupervised training), or some combination (semi-supervised learning, transfer learning, reinforcement training)) and testing.
  
> *让我们朝着AI训练师的方向进发吧！*

## 1: Regression
- Gradient descent: 只要可微分的function就可以用gradient descent找到local minima. 如果有多个参数，就对每个参数（每一个dimension）做偏微分。But in linear regression, the Loss function is *convex*.
- Regularization: Redefined the loss function to make the function more smooth (the functions with smaller $w_i$ are better, therefore are insensitive to noise). 
- Learning rate: we can specify customized learning rate for different parameters, e.g. *Adagrad Optimizer*.

## 2: Where dose the error come from
- There are two types of error : 1. from bias (the difference between the true function $\hat{f}$ and our estimated function $f^*$) and 2. from variance (the variance of the estimated functions).
  - Overfitting: Error from variance is too big. You should try to collect more training data, or use *regularization*.
  - Underfitting: Error from bias is too big. You should redesign your model (add more features or use a more complex model) to fit the training sets.

- Cross-validation: We should only use training set to choose your model, don't cherry-picking with the testing set. <img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-18%20at%2014.22.25.png" width="500"/>
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-18%20at%2014.34.06.png" width="500"/>

## 3: Gradient descent
- Adaptive learning rate: we can use adaptive learning rate to better approach the local minima. For example in the below figure, $\eta^t$ depends on the number of epochs $t$, and $\sigma^t$ depends on (all) the previous derivatives.
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-18%20at%2020.03.19.png" width="500"/>
The denominator in adagrad formula estimates the *second derivative* of the gradient. We need to account for the second derivative because the best step should also consider the second derivative (see below: although the points in $w2$ have bigger first derivatives than data points in $w1$, they in fact are more close to the local minima).
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2000.18.18.png" width="500"/>
- Stochastic gradient descent: Loss for only one sample, the advantage is it can update very frequently.
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2000.41.57.png" width="500"/>
- Feature scaling: Make different features have the same scaling will help finding the local minima more efficiently.
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2000.52.54.png" width="500"/>