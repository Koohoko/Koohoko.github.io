---
title: "Course notes: Machine Learning (Lee Hung-yi, 2016)"
date: 2022-12-17
permalink: /posts/2022-12-17/ml-ntu-2016
categories:
  - Course notes
tags:
  - Machine learning
toc: true
last_modified_at: 2022-12-17
---

This post records the notes when I learnt the [machine learning course from Lee Hung-yi](https://www.youtube.com/playlist?list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49). 

## 0: Introduction of Machine Learning
The framework of machine learning can be summarized into three steps:
  1. A set of functions (Model);
  2. Goodness of function $f$ (build a function to evaluate the functions in step 1, e.g. Loss function);
  3. Pick the best function (using training data (supervised learning, structured learning), or without training data (unsupervised training), or some combination (semi-supervised learning, transfer learning, reinforcement training)) and testing.
  
> *让我们朝着AI训练师的方向进发吧！*

## 1: Regression
- Gradient descent: 只要可微分的function就可以用gradient descent找到local minima. 如果有多个参数，就对每个参数（每一个dimension）做偏微分。But in linear regression, the Loss function is *convex*.
- Regularization: Redefined the loss function to make the function more smooth (the functions with smaller $w_i$ are better, therefore are insensitive to noise). 
- Learning rate: we can specify customized learning rate for different parameters, e.g. *Adagrad Optimizer*.

## 2: Where dose the error come from
- There are two types of error : 1. from bias (the difference between the true function $\hat{f}$ and our estimated function $f^*$) and 2. from variance (the variance of the estimated functions).
  - Overfitting: Error from variance is too big. You should try to collect more training data, or use *regularization*.
  - Underfitting: Error from bias is too big. You should redesign your model (add more features or use a more complex model) to fit the training sets.

- Cross-validation: We should only use training set to choose your model, don't cherry-picking with the testing set. <img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-18%20at%2014.22.25.webp" width="500"/>
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-18%20at%2014.34.06.webp" width="500"/>

## 3: Gradient descent
### Adaptive learning rate
We can use adaptive learning rate to better approach the local minima. For example in the below figure, $\eta^t$ depends on the number of epochs $t$, and $\sigma^t$ depends on (all) the previous derivatives.

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-18%20at%2020.03.19.webp" width="500"/>

The denominator in adagrad formula estimates the *second derivative* of the gradient. We need to account for the second derivative because the best step should also consider the second derivative (see below: although the points in $w2$ have bigger first derivatives than data points in $w1$, they in fact are more close to the local minima).

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2000.18.18.webp" width="500"/>

### Stochastic gradient descent
Loss for only one sample, the advantage is it can update very frequently.

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2000.41.57.webp" width="500"/>

### Feature scaling
Make different features have the same scaling will help finding the local minima more efficiently.

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2000.52.54.webp" width="500"/>

## 4. Classification
In classification problems, the outputs are class/categories. 

### Naive regression
To solve a classification problem, one way (probably in instinct) is to use the regression. Specifically, you can label one class as $1$ and the other class as $-1$, then find the best parameters $w_1$ and $w_2$ in the model $y=b+w_1x_1+w_2x_2$. However, there are some problem with this naive approach, that it is sensitive to outlier values, and it is hard to deal with multiple classes:

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2018.24.09.webp" width="500"/>

### Probability generative model
与生成模型(generative model)对应的是决断模型(descrimintive model). 类似regression, SVM这样的模型属于决断模型。而另一种思路是生成模型，即生成每个class的概率分布，然后对比新的feature在每个class的概率分布里面的可能性(e.g., $P(x|C_1)$)的大小。

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2021.06.04.webp" width="500"/>

三步建立概率生成模型

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2023.36.40.webp" width="500"/>

如果不同参数之间是independent的，这种情况下构成一个Naive Bayes Classifier. 
而当不同的class的概率分布share同一个covariance matrix的时候，这是一个"linear model" (2-D的时候, 判别线为直线). 2-D下, 当$\Sigma_1=\Sigma_2$时，其linearity可见下面的推导:

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2023.45.51.webp" width="500"/>

> *中间步骤省略*

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-19%20at%2023.53.42.webp" width="500"/>

## 5. Logistic Regression
Logistic Regression可以跟上面的Probability generative model里面的特殊情形(shared covariance matrix)联系起来,但logistic regression不假设任何概率分布. 本质上是一个通过sigmod function连接的linear regression.

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-20%20at%2014.04.25.webp" width="500"/>

### Step 1: Function set
The posterior probability $P(C_1|x_1)$ is a function set, which contains all combination of parameters $w$ and $b$. 

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-20%20at%2013.32.05.webp" width="500"/>

### Step 2: Goodness of the function
Using maximal likelihood, we can find the best function. The loss function of Logistic Regression is the sum of cross entropy between $f(x^n)$ and $\hat{y}^n$.

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-20%20at%2013.37.23.webp" width="500"/>
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-20%20at%2013.48.16.webp" width="500"/>

### Step 3: Find the best function
对$w_i$和$b_i$取偏微分, 用gradient descent.

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-20%20at%2014.00.40.webp" width="500"/>
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-20%20at%2014.03.07.webp" width="500"/>

### Why can't we use square error as the loss function
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-20%20at%2014.33.26.webp" width="500"/>
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-20%20at%2014.31.14.webp" width="500"/>

### Multi-class classification
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-20%20at%2014.59.08.webp" width="500"/>
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-20%20at%2014.59.22.webp" width="500"/>

### Limitation of logistic regression
The boundary is linear, thus it can not fit into some data. We can use feature transformation to cope with this. Specifically, when we can cascade logistic regression models, then it will be a simple neural network!

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-20%20at%2017.12.41.webp" width="500"/>

## 6. Brief introduction of Deep Learning
Neural network是一连串的matrix的operation, 所以用GPU可以方便计算。

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-21%20at%2000.30.41.webp" width="500"/>

Neural network的中间层可以视为feature transformation的过程, 最后的output layer一般接一个softmax, 即可作为[multi-class classifier](#multi-class-classification). Loss function也和multi-class classification的时候一摸一样，计算$f(x^n)$ and $\hat{y}^n$ 的cross entropy即可。

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-21%20at%2000.32.39.webp" width="500"/>

Find the best function的过程也是一样，用gradient descent即可. Backpropagation 是一个有效算微分的方法. 

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-21%20at%2000.45.28.webp" width="500"/>
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-21%20at%2000.46.21.webp" width="500"/>

## 7. Backpropagation
Backpropagation是一种用来高效求微分的方法.
先复习一下链式法则;)

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-21%20at%2016.15.26.webp" width="500"/>

$\frac{\partial{C}}{\partial{w}}$可以由$\frac{\partial{C}}{\partial{z}}$ (calculated by backward pass) 和$\frac{\partial{z}}{\partial{w}}$ (calculated by forward pass)计算.

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-21%20at%2018.04.14.webp" width="500"/>
<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-21%20at%2017.45.30.webp" width="500"/>

## 8. Keras

*为什么不是batch size越大越好？*
请注意neural network的问题并不少covex的，你如果不添加一点随机性（用小一点的batch size），那么模型很容易一下子就陷到local minima了。随机性是必要的，以便于离开一个saddle point或不是很深的local minima。（这跟做决策一样，既然我们无法预测未来，我们可能需要一点随机性:)）

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-22%20at%2000.48.22.webp" width="500"/>

GPU加速的秘诀在于用mini batch的时候，跟用stochastic gradient descent（一个sample一个batch），计算速度几乎是一样的。

<img src="/files/2022-12-17-ml-ntu-2016/Screenshot%202022-12-22%20at%2000.50.27.webp" width="500"/>

## 9. Tips for training DNN

